{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from load_data import read_data_sets\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "rel_vocab_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pre-processed data\n"
     ]
    }
   ],
   "source": [
    "datasets = read_data_sets(datafolder='/home/rupchap/data/NYT/',\n",
    "                          vocab_size=vocab_size, rel_vocab_size=rel_vocab_size,\n",
    "                          validation_size=5000, test_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of training examples from data\n",
    "num_examples = datasets.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph params\n",
    "# input_dimension = 1 vocab_size\n",
    "predict_classes = rel_vocab_size\n",
    "learning_rate = 0.01\n",
    "training_epochs = 5\n",
    "batch_size = 50\n",
    "display_step = 5\n",
    "embed_size = 30\n",
    "hidden_size = 5\n",
    "num_steps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(x):\n",
    "    print 'x:', x\n",
    "    # Embedding\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, embed_size])\n",
    "        inputs = tf.nn.embedding_lookup(embedding, x)\n",
    "    print 'inputs:', inputs\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(inputs):\n",
    "        \n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0) \n",
    "    \n",
    "    initial_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n",
    "    \n",
    "    print 'inputs:', inputs\n",
    "    \n",
    "    outputs, state = rnn.rnn(lstm_cell, inputs, initial_state)\n",
    "    output = tf.reshape(tf.concat(1, outputs), [-1, hidden_size])\n",
    "    print 'output:', output\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [hidden_size, predict_classes])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [predict_classes])\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    print 'logits:', logits\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(logits, y):\n",
    "    dot_product = y * tf.log(logits)\n",
    "    y = tf.to_int64(y)\n",
    "    xentropy = -tf.reduce_sum(dot_product, reduction_indices=1)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    tf.scalar_summary('cost', cost)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    correct_prediction = tf.equal(tf.arg_max(output, dimension=1),\n",
    "                                  tf.arg_max(y, dimension=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num steps: 5\nx: Tensor(\"Placeholder:0\", shape=(?, 104), dtype=int32)\ninputs: Tensor(\"embedding_lookup:0\", shape=(?, 104, 30), dtype=float32, device=/device:CPU:0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of ways to split should evenly divide the split dimension but got split_dim 1 (size = 104) and num_split 5",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-182-d2fba225b19f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'num steps:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-178-216760a4388f>\u001b[0m in \u001b[0;36minference\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minput_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'inputs:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rupchap/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36msplit\u001b[1;34m(split_dim, num_split, value, name)\u001b[0m\n\u001b[0;32m    500\u001b[0m                               \u001b[0mnum_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m                               \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m                               name=name)\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rupchap/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36m_split\u001b[1;34m(split_dim, value, num_split, name)\u001b[0m\n\u001b[0;32m   1016\u001b[0m   \"\"\"\n\u001b[0;32m   1017\u001b[0m   return _op_def_lib.apply_op(\"Split\", split_dim=split_dim, value=value,\n\u001b[1;32m-> 1018\u001b[1;33m                               num_split=num_split, name=name)\n\u001b[0m\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rupchap/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    653\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    654\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    656\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_Restructure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_n_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rupchap/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2040\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2041\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2042\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2043\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rupchap/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1526\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[0;32m   1527\u001b[0m                          % op.type)\n\u001b[1;32m-> 1528\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1529\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m/home/rupchap/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36m_SplitShape\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1153\u001b[0m           \u001b[1;34m\"Number of ways to split should evenly divide the split \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m           \u001b[1;34m\"dimension but got split_dim %d (size = %d) and num_split %d\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1155\u001b[1;33m           (split_dim, input_shape[split_dim].value, num_split))\n\u001b[0m\u001b[0;32m   1156\u001b[0m     \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msplit_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m     \u001b[0msize_in_split_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit_dim\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mnum_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of ways to split should evenly divide the split dimension but got split_dim 1 (size = 104) and num_split 5"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # TODO: will currently pad to max sentence length in training\n",
    "    # Consider modifying to use bucketed models.\n",
    "    max_sentence_length = max([len(sentence) for sentence in datasets.train.sentences])\n",
    "    \n",
    "    x = tf.placeholder(dtype='int32', shape=[None, max_sentence_length])\n",
    "    y = tf.placeholder(dtype='int32', shape=[None, predict_classes])\n",
    "    \n",
    "    print 'num steps:', num_steps\n",
    "    inputs = embed(x)\n",
    "    logits = inference(inputs)\n",
    "    cost = loss(logits, y)\n",
    "    \n",
    "    global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "    \n",
    "    train_op = training(cost, global_step)\n",
    "    \n",
    "    eval_op = evaluate(output, y)\n",
    "    \n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    \n",
    "    summary_writer = tf.train.SummaryWriter('tflogs/', graph_def=sess.graph_def)\n",
    "    \n",
    "    init_op = tf.initialize_all_variables()\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # Training loop over epochs\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        num_batches = int(num_examples/batch_size)\n",
    "        # Loop over batches\n",
    "        for i in range(num_batches):\n",
    "            batch_x, batch_y = datasets.train.next_batch(batch_size)\n",
    "            \n",
    "            # pad sentences up to length of longest sentence in batch\n",
    "            # ToDo: fix this to use buckets instead for efficiency\n",
    "            PAD_ID = 1\n",
    "            for sentence in batch_x:\n",
    "                sentence += [PAD_ID] * (max_sentence_length - len(sentence))\n",
    "            print batch_x\n",
    "            \n",
    "            # train on batch data\n",
    "            feed_dict = {x: batch_x, y: batch_y}\n",
    "            sess.run(train_op, feed_dict=feed_dict)\n",
    "            \n",
    "            # compute average loss\n",
    "            minibatch_cost = sess.run(cost, feed_dict=feed_dict)\n",
    "            avg_cost += minibatch_cost/num_batches\n",
    "            \n",
    "            # display logs\n",
    "            summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, global_step=sess.run(global_step))\n",
    "                       \n",
    "        print 'Optimisation complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}